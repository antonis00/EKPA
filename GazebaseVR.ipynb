{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21363a25669847c099f9075d28537761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_120bcff696d64e3dbf881ed2d82ae790",
              "IPY_MODEL_259c5077bae34194bf104539941ca9e3",
              "IPY_MODEL_c83dc2896aa04b49b0f2a4e420837e85"
            ],
            "layout": "IPY_MODEL_63a634ca770042818c335286445082bf"
          }
        },
        "120bcff696d64e3dbf881ed2d82ae790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4101ccf52d549eca7d05f841d081fc3",
            "placeholder": "​",
            "style": "IPY_MODEL_b5bbe909289d4dee8fded00915d6435c",
            "value": "gazebasevr.zip: 100%"
          }
        },
        "259c5077bae34194bf104539941ca9e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a395ca77c4f0466fad39a14dc6ee8834",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73243eb4109541e99191ec620779ccf2",
            "value": 1
          }
        },
        "c83dc2896aa04b49b0f2a4e420837e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b20f166ba5914d1c9a9227cc668f89bd",
            "placeholder": "​",
            "style": "IPY_MODEL_17e00c1fdba549f59c26fbd933c2ddeb",
            "value": " 2.30G/2.30G [03:08&lt;00:00, 11.6MB/s]"
          }
        },
        "63a634ca770042818c335286445082bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4101ccf52d549eca7d05f841d081fc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5bbe909289d4dee8fded00915d6435c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a395ca77c4f0466fad39a14dc6ee8834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "73243eb4109541e99191ec620779ccf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b20f166ba5914d1c9a9227cc668f89bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17e00c1fdba549f59c26fbd933c2ddeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonis00/EKPA/blob/main/GazebaseVR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymovements\n",
        "import pymovements as pm\n",
        "dataset = pm.Dataset(\"GazeBaseVR\", path='data/GazeBaseVR')\n",
        "dataset.download()\n",
        "dataset.load()\n"
      ],
      "metadata": {
        "id": "OPB-fEruCQC5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518,
          "referenced_widgets": [
            "21363a25669847c099f9075d28537761",
            "120bcff696d64e3dbf881ed2d82ae790",
            "259c5077bae34194bf104539941ca9e3",
            "c83dc2896aa04b49b0f2a4e420837e85",
            "63a634ca770042818c335286445082bf",
            "c4101ccf52d549eca7d05f841d081fc3",
            "b5bbe909289d4dee8fded00915d6435c",
            "a395ca77c4f0466fad39a14dc6ee8834",
            "73243eb4109541e99191ec620779ccf2",
            "b20f166ba5914d1c9a9227cc668f89bd",
            "17e00c1fdba549f59c26fbd933c2ddeb"
          ]
        },
        "outputId": "3be11546-f3c6-4cb9-92cd-353c336d812c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymovements in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: matplotlib<3.9,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from pymovements) (3.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pymovements) (1.26.4)\n",
            "Requirement already satisfied: pandas<3,>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pymovements) (2.1.4)\n",
            "Requirement already satisfied: polars<0.20.3,>=0.20.1 in /usr/local/lib/python3.10/dist-packages (from pymovements) (0.20.2)\n",
            "Requirement already satisfied: pyarrow<16,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from pymovements) (14.0.2)\n",
            "Requirement already satisfied: pyopenssl<25.0.0,>=16.0.0 in /usr/local/lib/python3.10/dist-packages (from pymovements) (24.2.1)\n",
            "Requirement already satisfied: scipy<2,>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from pymovements) (1.13.1)\n",
            "Requirement already satisfied: tqdm<5,>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from pymovements) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pymovements) (4.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.8.0->pymovements) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.8.0->pymovements) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.8.0->pymovements) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.8.0->pymovements) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.8.0->pymovements) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.8.0->pymovements) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.8.0->pymovements) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.8.0->pymovements) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=2.1.4->pymovements) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=2.1.4->pymovements) (2024.1)\n",
            "Requirement already satisfied: cryptography<44,>=41.0.5 in /usr/local/lib/python3.10/dist-packages (from pyopenssl<25.0.0,>=16.0.0->pymovements) (42.0.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<44,>=41.0.5->pyopenssl<25.0.0,>=16.0.0->pymovements) (1.17.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<3.9,>=3.8.0->pymovements) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyopenssl<25.0.0,>=16.0.0->pymovements) (2.22)\n",
            "Downloading https://figshare.com/ndownloader/files/38844024 to data/GazeBaseVR/downloads/gazebasevr.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gazebasevr.zip: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21363a25669847c099f9075d28537761"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking integrity of gazebasevr.zip\n",
            "Extracting gazebasevr.zip to data/GazeBaseVR/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eye Movement-Based User Identification\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook demonstrates a machine learning approach to identify users based on their eye movement patterns across different tasks. We use data collected from participants performing five distinct eye-tracking (ET) tasks:\n",
        "\n",
        "1. Vergence task (VRG)\n",
        "2. Horizontal smooth pursuit task (PUR)\n",
        "3. Video-viewing task (VID)\n",
        "4. Self-paced reading task (TEX)\n",
        "5. Random oblique saccade task (RAN)\n",
        "\n",
        "## Data Processing and Feature Extraction\n",
        "\n",
        "We extract a comprehensive set of features from the eye-tracking data for each task. The features are calculated both globally and for multiple time-based portions of each task.\n",
        "\n",
        "### Velocity Calculation\n",
        "\n",
        "Before feature extraction, we calculate the velocity of eye movements using the Savitzky-Golay filter:\n",
        " dataset.pos2vel(method='savitzky_golay', degree=2, window_length=7)\n",
        "*italicised text*\n",
        " This method applies a Savitzky-Golay filter to smooth the position data and calculate velocities. It uses a polynomial of degree 2 and a window length of 7 samples, which helps to reduce noise while preserving the underlying signal characteristics.\n",
        "\n",
        "### Extracted Features\n",
        "\n",
        "For each portion and globally:\n",
        "\n",
        "1. **Velocity Statistics**:\n",
        "   - Mean velocity\n",
        "   - Standard deviation of velocity\n",
        "   - Maximum velocity\n",
        "   - Skewness of velocity distribution\n",
        "   - Kurtosis of velocity distribution\n",
        "\n",
        "2. **Position Statistics**:\n",
        "   - Mean X and Y positions\n",
        "   - Standard deviation of X and Y positions\n",
        "\n",
        "3. **Target Position** (if available):\n",
        "   - Mean X and Y target positions\n",
        "   - Standard deviation of X and Y target positions\n",
        "\n",
        "4. **Saccade and Fixation Metrics**:\n",
        "   - Saccade rate (proportion of samples above saccade threshold)\n",
        "   - Fixation rate (proportion of samples below fixation threshold)\n",
        "\n",
        "5. **Task Duration**:\n",
        "   - Total number of samples in the task\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "We use a Multi-Layer Perceptron (MLP) classifier with the following structure:\n",
        "- Input layer (size depends on the number of features)\n",
        "- Hidden layers: 64 units, 32 units, 16 units\n",
        "- Output layer (size equals the number of unique users)\n",
        "- A skip connection from input to output for improved learning\n",
        "\n",
        "## Training Process\n",
        "\n",
        "The model is trained using:\n",
        "- Adam optimizer with a learning rate of 0.05\n",
        "- Cross-entropy loss function\n",
        "- 200 epochs\n",
        "- Batch size of 256\n",
        "\n",
        "## Results\n",
        "\n",
        "The overall test accuracy achieved is 68%, indicating that the model can correctly identify users based on their eye movements in 68% of cases.\n",
        "\n",
        "### Task-Specific Accuracies\n",
        "\n",
        "1. Video-viewing task (VID): 93.03%\n",
        "2. Random oblique saccade task (RAN): 72.11%\n",
        "3. Horizontal smooth pursuit task (PUR): 68.33%\n",
        "4. Self-paced reading task (TEX): 57.37%\n",
        "5. Vergence task (VRG): 43.82%\n",
        "\n",
        "## Analysis\n",
        "\n",
        "The results show that different tasks have varying levels of effectiveness in identifying users:\n",
        "\n",
        "1. The video-viewing task is the most effective, with an impressive 93.03% accuracy. This suggests that how people watch videos is highly individual and consistent.\n",
        "\n",
        "2. The random oblique saccade task and horizontal smooth pursuit task show moderate effectiveness, with accuracies above the overall average.\n",
        "\n",
        "3. The self-paced reading task and vergence task are less effective for user identification, with accuracies below the overall average.\n",
        "\n",
        "These findings indicate that complex, naturalistic tasks like video viewing may be more suitable for eye movement-based user identification compared to simpler, controlled tasks like vergence exercises.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This study demonstrates the potential of using eye movement patterns for user identification, with varying degrees of success across different tasks. The use of the Savitzky-Golay filter for velocity calculation provides a robust basis for feature extraction. Future work could focus on optimizing feature extraction for the most effective tasks or combining data from multiple tasks to improve overall accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "bCCv-WYaCFMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "position_columns = ['x_left', 'y_left', 'x_right', 'y_right']\n",
        "\n",
        "# Calculate velocity\n",
        "dataset.pos2vel( method='savitzky_golay', degree=2, window_length=7)\n",
        "print(dataset.gaze[5].frame)"
      ],
      "metadata": {
        "id": "dxyit6PtCRtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import polars as pl\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_features(gaze_data, n_portions=10, set_type='train'):\n",
        "    features = []\n",
        "    y = []\n",
        "    task_names = []\n",
        "\n",
        "    for gaze in tqdm(gaze_data):\n",
        "        frame = gaze.frame\n",
        "        # Create a temporary index column\n",
        "        frame = frame.with_row_count(\"temp_index\")\n",
        "\n",
        "        if set_type == 'train':\n",
        "            frame = frame.filter(pl.col('temp_index') % 3 != 2)\n",
        "        elif set_type == 'test':\n",
        "            frame = frame.filter(pl.col('temp_index') % 3 == 2)\n",
        "\n",
        "        # Drop the temporary index column\n",
        "        frame = frame.drop('temp_index')\n",
        "\n",
        "        total_samples = frame.shape[0]\n",
        "        portion_size = total_samples // n_portions\n",
        "\n",
        "        # Initialize lists to store portion-wise statistics\n",
        "        portion_features = []\n",
        "\n",
        "        # Calculate statistics for each portion\n",
        "        for i in range(n_portions):\n",
        "            start_idx = i * portion_size\n",
        "            end_idx = (i + 1) * portion_size if i < n_portions - 1 else total_samples\n",
        "\n",
        "            portion = frame.slice(start_idx, end_idx - start_idx)\n",
        "\n",
        "            # Velocity statistics\n",
        "            velocities = portion['velocity'].apply(lambda x: np.linalg.norm(x)).to_numpy()\n",
        "\n",
        "            # Position statistics\n",
        "            positions = np.vstack(portion['position'].to_numpy())\n",
        "\n",
        "            portion_stats = [\n",
        "                np.mean(velocities),\n",
        "                np.std(velocities),\n",
        "                np.max(velocities),\n",
        "                *np.mean(positions, axis=0),\n",
        "                *np.std(positions, axis=0),\n",
        "                stats.skew(velocities),\n",
        "                stats.kurtosis(velocities)\n",
        "            ]\n",
        "\n",
        "            # Add any other relevant statistics from other columns\n",
        "            if 'x_target_pos' in portion.columns and 'y_target_pos' in portion.columns:\n",
        "                portion_stats.extend([\n",
        "                    portion['x_target_pos'].mean(),\n",
        "                    portion['y_target_pos'].mean(),\n",
        "                    portion['x_target_pos'].std(),\n",
        "                    portion['y_target_pos'].std()\n",
        "                ])\n",
        "\n",
        "            portion_features.extend(portion_stats)\n",
        "\n",
        "        # Global features\n",
        "        velocities = frame['velocity'].apply(lambda x: np.linalg.norm(x)).to_numpy()\n",
        "        positions = np.vstack(frame['position'].to_numpy())\n",
        "\n",
        "        global_features = [\n",
        "            np.mean(velocities),\n",
        "            np.std(velocities),\n",
        "            np.max(velocities),\n",
        "            *np.mean(positions, axis=0),\n",
        "            *np.std(positions, axis=0),\n",
        "            stats.skew(velocities),\n",
        "            stats.kurtosis(velocities)\n",
        "        ]\n",
        "\n",
        "        # Saccade and fixation features\n",
        "        saccade_threshold = np.mean(velocities) + 2 * np.std(velocities)\n",
        "        fixation_threshold = np.mean(velocities) - np.std(velocities)\n",
        "        saccade_rate = np.sum(velocities > saccade_threshold) / total_samples\n",
        "        fixation_rate = np.sum(velocities < fixation_threshold) / total_samples\n",
        "\n",
        "        # Task-specific features\n",
        "        task_name = frame['task_name'][0]\n",
        "        task_duration = total_samples  # Assuming constant sampling rate\n",
        "\n",
        "        # Combine all features\n",
        "        feature_vector = portion_features + global_features + [saccade_rate, fixation_rate, task_duration]\n",
        "\n",
        "        features.append(feature_vector)\n",
        "        y.append(frame['subject_id'][0])\n",
        "        task_names.append(task_name)\n",
        "\n",
        "    return np.array(features), np.array(y), np.array(task_names)\n",
        "\n",
        "# Extract features for training set (1st and 2nd rows)\n",
        "X_train, y_train, task_names_train = extract_features(dataset.gaze, n_portions=100, set_type='train')\n",
        "\n",
        "# Extract features for test set (every 3rd row)\n",
        "X_test, y_test, task_names_test = extract_features(dataset.gaze, n_portions=100, set_type='test')\n",
        "\n",
        "# Save the features\n",
        "np.save('X_train.npy', X_train)\n",
        "np.save('y_train.npy', y_train)\n",
        "np.save('task_names_train.npy', task_names_train)\n",
        "\n",
        "np.save('X_test.npy', X_test)\n",
        "np.save('y_test.npy', y_test)\n",
        "np.save('task_names_test.npy', task_names_test)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "rIrukhj5CZ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class EyeMovementDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 16)\n",
        "        self.fc5 = nn.Linear(16, num_classes)\n",
        "        self.fc6 = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.fc1(x))\n",
        "        out = torch.relu(self.fc2(out))\n",
        "        out = torch.relu(self.fc3(out))\n",
        "        out = self.fc5(out)\n",
        "        d = self.fc6(x)\n",
        "        return out + 10 * d\n",
        "\n",
        "\n",
        "def evaluate_task_accuracy(\n",
        "    model, X_test, y_test, task_names_test, label_encoder, specific_task=None\n",
        "):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    predicted = predicted.cpu().numpy()\n",
        "    probabilities = probabilities.cpu().numpy()\n",
        "\n",
        "    task_results = defaultdict(lambda: {\"true\": [], \"pred\": [], \"prob\": []})\n",
        "    for true, pred, prob, task in zip(\n",
        "        y_test_encoded, predicted, probabilities, task_names_test\n",
        "    ):\n",
        "        task_results[task][\"true\"].append(true)\n",
        "        task_results[task][\"pred\"].append(pred)\n",
        "        task_results[task][\"prob\"].append(prob[true])\n",
        "\n",
        "    task_accuracies = {}\n",
        "    for task, results in task_results.items():\n",
        "        if specific_task is None or task == specific_task:\n",
        "            accuracy = accuracy_score(results[\"true\"], results[\"pred\"])\n",
        "            avg_probability = np.mean(results[\"prob\"])\n",
        "            task_accuracies[task] = {\n",
        "                \"accuracy\": accuracy,\n",
        "                \"avg_probability\": avg_probability,\n",
        "            }\n",
        "\n",
        "    if specific_task:\n",
        "        if specific_task in task_accuracies:\n",
        "            print(f\"Task: {specific_task}\")\n",
        "            print(f\"Accuracy: {task_accuracies[specific_task]['accuracy']:.2%}\")\n",
        "            print(\n",
        "                f\"Average Probability: {task_accuracies[specific_task]['avg_probability']:.4f}\"\n",
        "            )\n",
        "        else:\n",
        "            print(f\"Task {specific_task} not found in the test set.\")\n",
        "    else:\n",
        "        sorted_tasks = sorted(\n",
        "            task_accuracies.items(), key=lambda x: x[1][\"accuracy\"], reverse=True\n",
        "        )\n",
        "        print(\"Task Accuracies and Average Probabilities:\")\n",
        "        for task, metrics in sorted_tasks:\n",
        "            print(\n",
        "                f\"{task}: Accuracy: {metrics['accuracy']:.2%}, Avg Probability: {metrics['avg_probability']:.4f}\"\n",
        "            )\n",
        "\n",
        "    return task_accuracies\n",
        "\n",
        "\n",
        "# Load the data\n",
        "X_train = np.load(\"X_train.npy\", allow_pickle=True).astype(np.float32)\n",
        "y_train = np.load(\"y_train.npy\", allow_pickle=True)\n",
        "X_test = np.load(\"X_test.npy\", allow_pickle=True).astype(np.float32)\n",
        "y_test = np.load(\"y_test.npy\", allow_pickle=True)\n",
        "task_names_test = np.load(\"task_names_test.npy\", allow_pickle=True)\n",
        "\n",
        "# Get unique task names\n",
        "unique_task_names = np.unique(task_names_test)\n",
        "print(\"Available tasks:\", unique_task_names)\n",
        "\n",
        "# Limit to top 100 classes\n",
        "class_counts = Counter(y_train)\n",
        "top_100_classes = set([cls for cls, _ in class_counts.most_common(1000)])\n",
        "\n",
        "# Filter data to keep only top 100 classes\n",
        "train_mask = np.isin(y_train, list(top_100_classes))\n",
        "test_mask = np.isin(y_test, list(top_100_classes))\n",
        "\n",
        "X_train = X_train[train_mask]\n",
        "y_train = y_train[train_mask]\n",
        "X_test = X_test[test_mask]\n",
        "y_test = y_test[test_mask]\n",
        "task_names_test = task_names_test[test_mask]\n",
        "# Check for NaN or infinite values\n",
        "X_train = np.nan_to_num(X_train, nan=0, posinf=1e6, neginf=-1e6)\n",
        "X_test = np.nan_to_num(X_test, nan=0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[1]\n",
        "num_classes = len(np.unique(y_train_encoded))\n",
        "model = MLPClassifier(input_size, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Check if a trained model exists\n",
        "model_path = \"model.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Loading existing model...\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "else:\n",
        "    print(\"Training new model...\")\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = EyeMovementDataset(X_train, y_train_encoded)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 200\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in tqdm(\n",
        "            train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"\n",
        "        ):\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_dataloader):.4f}\"\n",
        "        )\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()\n",
        "test_dataset = EyeMovementDataset(X_test, y_test_encoded)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_X, batch_y in test_dataloader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += batch_y.size(0)\n",
        "        correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "    print(f\"Overall Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "chosen_task = \"3_VID\"\n",
        "task_accuracies = evaluate_task_accuracy(\n",
        "    model, X_test, y_test, task_names_test, label_encoder, chosen_task\n",
        ")\n",
        "\n",
        "\n",
        "# Function to predict on user data\n",
        "def predict_on_user_data(user_data, model, label_encoder, task_name):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Ensure user_data is 2D (add batch dimension if necessary)\n",
        "    if user_data.ndim == 1:\n",
        "        user_data = user_data.reshape(1, -1)\n",
        "\n",
        "    user_data_tensor = torch.FloatTensor(user_data).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(user_data_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    predicted = predicted.cpu().numpy()\n",
        "    probabilities = probabilities.cpu().numpy()\n",
        "\n",
        "    # Get index of highest probability class\n",
        "    indx = np.argmax(probabilities[0])\n",
        "    return probabilities[0][indx], task_names_test[indx]\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "user_data = X_test[1]  # Assuming this is a single user's data\n",
        "\n",
        "probability, name = predict_on_user_data(user_data, model, label_encoder, chosen_task)\n",
        "print(f\"Predicted Task for chosen user: {name} with probability {probability:.4f}\")\n"
      ],
      "metadata": {
        "id": "cw8G01BrCdDR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}